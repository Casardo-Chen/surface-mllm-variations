{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import uuid\n",
    "import pprint as pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages (uncomment the line below if running in a new environment)\n",
    "# !pip install sentence-transformers hdbscan\n",
    "\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON data from map.json\n",
    "with open(\"map.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Custom cosine distance function\n",
    "def cosine_distance(u, v):\n",
    "    # Ensure inputs are numpy arrays\n",
    "    u, v = np.array(u), np.array(v)\n",
    "    # Compute cosine distance: 1 - cosine similarity\n",
    "    return 1 - np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "\n",
    "def get_cluster(data):\n",
    "    # Extract the \"fact\" values for embedding\n",
    "    facts = [item[\"fact\"] for item in data]\n",
    "\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Compute embeddings for each fact (converting to numpy arrays)\n",
    "    embeddings = model.encode(facts, convert_to_numpy=True)\n",
    "\n",
    "    embeddings = normalize(embeddings)\n",
    "\n",
    "\n",
    "    # Cluster the embeddings using HDBSCAN.\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=2,metric= cosine_distance, cluster_selection_epsilon=0.2)\n",
    "    cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "    # Organize the items by their assigned cluster\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        # Initialize list for this label if not seen before\n",
    "        clusters.setdefault(label, []).append(data[idx])\n",
    "\n",
    "    # Re-map cluster labels to sequential cluster_id values starting from 1.\n",
    "    # If any items are marked as noise (label == -1), they will be assigned individual cluster_ids.\n",
    "    final_clusters = []\n",
    "    next_cluster_id = 1\n",
    "\n",
    "    # First, handle non-noise clusters\n",
    "    for label in sorted(clusters.keys()):\n",
    "        if label != -1:\n",
    "            final_clusters.append({\n",
    "                \"cluster_id\": next_cluster_id,\n",
    "                \"atomic facts\": clusters[label]\n",
    "            })\n",
    "            next_cluster_id += 1\n",
    "\n",
    "    # Then, assign each noise point its own cluster (if any)\n",
    "    if -1 in clusters:\n",
    "        for item in clusters[-1]:\n",
    "            final_clusters.append({\n",
    "                \"cluster_id\": next_cluster_id,\n",
    "                \"atomic facts\": [item]\n",
    "            })\n",
    "            next_cluster_id += 1\n",
    "\n",
    "\n",
    "    # Print the final clustered result in JSON format\n",
    "    with open(\"clustered.json\", \"w\") as f:\n",
    "        json.dump(final_clusters, f, indent=2)\n",
    "\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 9/9 [00:01<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 92 clusters\n"
     ]
    }
   ],
   "source": [
    "res = get_cluster(data)\n",
    "with open(\"clustered_test.json\", \"w\") as f:\n",
    "    json.dump(res, f, indent=2)\n",
    "\n",
    "print(f\"there are {len(res)} clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blurry graph\n",
    "# image = \"https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_val_00000943.jpg\"\n",
    "# prompt = \"Describe the information shown in this screen\"\n",
    "\n",
    "# Art\n",
    "image = \"https://m.media-amazon.com/images/I/71jD1RoJ7jL._AC_UL320_.jpg\"\n",
    "prompt = \"Describe the art for me. I am thinking of purchasing it and put it in my living room. What do you think?\"\n",
    "\n",
    "\n",
    "\n",
    "# models = [\"gpt\",\"gemini\",\"claude\"]\n",
    "# models = [\"gpt\",\"gemini\"]\n",
    "models = [\"gpt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 1, there are 9 clusters. In total, there are 28 atomic facts, ratio is 0.32142857142857145\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 2, there are 20 clusters. In total, there are 52 atomic facts, ratio is 0.38461538461538464\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 3, there are 30 clusters. In total, there are 67 atomic facts, ratio is 0.44776119402985076\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 4, there are 31 clusters. In total, there are 91 atomic facts, ratio is 0.34065934065934067\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 5, there are 47 clusters. In total, there are 113 atomic facts, ratio is 0.415929203539823\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 6, there are 59 clusters. In total, there are 143 atomic facts, ratio is 0.4125874125874126\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 7, there are 42 clusters. In total, there are 142 atomic facts, ratio is 0.29577464788732394\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 8, there are 81 clusters. In total, there are 158 atomic facts, ratio is 0.5126582278481012\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 7/7 [00:01<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 9, there are 105 clusters. In total, there are 217 atomic facts, ratio is 0.4838709677419355\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:Descriptions generated\n",
      "INFO:pipeline:Descriptions broken down\n",
      "INFO:pipeline:Atomic facts broken down\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 7/7 [00:01<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for trial size 10, there are 97 clusters. In total, there are 205 atomic facts, ratio is 0.47317073170731705\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    _, res = pipeline.variation_generation(image, i, models, \"original\", prompt, \"./\", source=\"url\")\n",
    "    final_clusters = get_cluster(res)\n",
    "    print (f\"for trial size {i}, there are {len(final_clusters)} clusters. In total, there are {len(res)} atomic facts, ratio is {len(final_clusters)/len(res)}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  5.30it/s]\n"
     ]
    }
   ],
   "source": [
    "final_clusters = get_cluster(res)\n",
    "\n",
    "# save the final clusters\n",
    "with open(\"final_clustered.json\", \"w\") as f:\n",
    "    json.dump(final_clusters, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only get response id, description, and model from the description.json and store it in a new json file\n",
    "\n",
    "with open(\"./data/map/descriptions.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "new_data = []\n",
    "for item in data.values():\n",
    "    new_data.append({\n",
    "        \"response_id\": item[\"id\"],\n",
    "        \"description\": item[\"description\"],\n",
    "        \"model\": item[\"model\"]\n",
    "    })\n",
    "\n",
    "with open(\"description_map.json\", \"w\") as f:\n",
    "    json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm-variation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
